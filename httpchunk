#!/usr/bin/env bash
set -euo pipefail

usage() {
  cat <<'EOF'
httpchunk - Download a single HTTP/HTTPS URL into numbered chunk files using Range requests.

State model (files in current directory):
  <name>.XXXXXX.tmp   -> in progress
  <name>.XXXXXX.part  -> completed chunk
  <name>.XXXXXX.done  -> completion marker (source of truth)

Usage:
  httpchunk URL [options]

Options:
  -c CHUNK_SIZE     Chunk size (K, M, G decimal). Default: 100M
  -x PROXY          curl proxy URL (optional, e.g. socks5h://127.0.0.1:9050)
  -r RETRIES        Retries per request. Default: 10
  --no-head         Skip HEAD (requires --size)
  --size BYTES      Total size in bytes (required if --no-head)
  --index-width N   Digits for chunk index. Default: 6
  --jobs N          Concurrent chunks. Default: 1
  --force           Re-download even if .done exists
  --post-part CMD   Run after a chunk completes.
                    Placeholders: {part} {done} {idx} {base}
  --assemble FILE   Concatenate all *.part into FILE
  --help            Show this help
EOF
}

die() { echo "error: $*" >&2; exit 1; }

parse_size() {
  case "$1" in
    *K) echo $(( ${1%K} * 1000 ));;
    *M) echo $(( ${1%M} * 1000 * 1000 ));;
    *G) echo $(( ${1%G} * 1000 * 1000 * 1000 ));;
    *)  echo "$1";;
  esac
}

# ---------------- defaults ----------------
CHUNK_SIZE_STR="100M"
PROXY=""
RETRIES=10
NO_HEAD=0
SIZE=""
INDEX_WIDTH=6
FORCE=0
JOBS=1
POST_PART=""
ASSEMBLE=""

# ---------------- args ----------------
[[ $# -lt 1 ]] && usage && exit 1
URL="$1"; shift

while [[ $# -gt 0 ]]; do
  case "$1" in
    -c) CHUNK_SIZE_STR="$2"; shift 2;;
    -x) PROXY="$2"; shift 2;;
    -r) RETRIES="$2"; shift 2;;
    --no-head) NO_HEAD=1; shift;;
    --size) SIZE="$2"; shift 2;;
    --index-width) INDEX_WIDTH="$2"; shift 2;;
    --jobs) JOBS="$2"; shift 2;;
    --force) FORCE=1; shift;;
    --post-part) POST_PART="$2"; shift 2;;
    --assemble) ASSEMBLE="$2"; shift 2;;
    --help|-h) usage; exit 0;;
    *) die "unknown argument: $1";;
  esac
done

# ---------------- filename prefix ----------------
PREFIX="${URL%%[\?#]*}"
PREFIX="${PREFIX##*/}"
[[ -z "$PREFIX" ]] && PREFIX="download"

CHUNK_SIZE=$(parse_size "$CHUNK_SIZE_STR")
PAD="%0${INDEX_WIDTH}d"

# ---------------- curl opts ----------------
CURL_OPTS=(
  --fail
  --location
  --retry "$RETRIES"
  --retry-delay 2
  --retry-connrefused
  --speed-time 30
  --speed-limit 1024
)
[[ -n "$PROXY" ]] && CURL_OPTS+=( --proxy "$PROXY" )

# ---------------- get size ----------------
if [[ -z "$SIZE" ]]; then
  [[ "$NO_HEAD" -eq 1 ]] && die "--no-head requires --size"
  SIZE=$(curl "${CURL_OPTS[@]}" -sI "$URL" \
    | awk -F': ' 'tolower($1)=="content-length"{print $2}' \
    | tr -d '\r')
  [[ -z "$SIZE" ]] && die "could not determine Content-Length"
fi

PARTS=$(( (SIZE + CHUNK_SIZE - 1) / CHUNK_SIZE ))

# ---------------- progress state ----------------
PROGRESS_FILE=".${PREFIX}.progress"
LOCK_FILE=".${PREFIX}.progress.lock"

# inicializar contador
if [[ ! -f "$PROGRESS_FILE" || "$FORCE" -eq 1 ]]; then
  echo 0 > "$PROGRESS_FILE"
else
  # contar .done existentes
  ls "${PREFIX}."*.done 2>/dev/null | wc -l > "$PROGRESS_FILE"
fi

inc_done() {
  (
    flock 9
    done_now=$(<"$PROGRESS_FILE")
    done_now=$((done_now + 1))
    echo "$done_now" > "$PROGRESS_FILE"
    printf '[%d/%d] chunks completed\n' "$done_now" "$PARTS"
  ) 9>"$LOCK_FILE"
}

echo "URL        : $URL"
echo "File       : $PREFIX"
echo "Size       : $SIZE"
echo "Chunk size : $CHUNK_SIZE"
echo "Chunks     : $PARTS"
echo "Jobs       : $JOBS"
[[ -n "$PROXY" ]] && echo "Proxy      : $PROXY" || echo "Proxy      : none"
echo

run_post_part() {
  local cmd="$1" part="$2" donef="$3" idx="$4" base="$5"
  [[ -z "$cmd" ]] && return 0

  cmd="${cmd//\{part\}/$part}"
  cmd="${cmd//\{done\}/$donef}"
  cmd="${cmd//\{idx\}/$idx}"
  cmd="${cmd//\{base\}/$base}"

  ( eval "$cmd" )
}

download_one() {
  local i="$1"
  local start end need base tmp part donef have range_start

  start=$(( i * CHUNK_SIZE ))
  end=$(( start + CHUNK_SIZE - 1 ))
  [[ "$end" -ge "$((SIZE-1))" ]] && end=$((SIZE-1))
  need=$(( end - start + 1 ))

  base=$(printf "$PAD" "$i")
  tmp="${PREFIX}.${base}.tmp"
  part="${PREFIX}.${base}.part"
  donef="${PREFIX}.${base}.done"

  if [[ "$FORCE" -eq 1 ]]; then
    rm -f "$tmp" "$part" "$donef"
  fi

  [[ -f "$donef" ]] && return 0

  while true; do
    have=0
    [[ -f "$tmp" ]] && have=$(stat -c %s "$tmp")

    if [[ "$have" -ge "$need" ]]; then
      mv -f "$tmp" "$part" 2>/dev/null || true
      printf '%s\n' "$start-$end" > "$donef"
      run_post_part "$POST_PART" "$part" "$donef" "$i" "$base" || true
      inc_done
      return 0
    fi

    range_start=$(( start + have ))
    printf 'â†’ chunk %d/%d (%s) bytes %d-%d\n' "$((i+1))" "$PARTS" "$base" "$range_start" "$end"

    if ! curl "${CURL_OPTS[@]}" --range "${range_start}-${end}" "$URL" >> "$tmp"; then
      sleep 3
    fi
  done
}

# ---------------- job pool ----------------
active=0
for ((i=0; i<PARTS; i++)); do
  download_one "$i" &
  active=$((active + 1))
  if [[ "$active" -ge "$JOBS" ]]; then
    wait -n || true
    active=$((active - 1))
  fi
done

wait || true
echo "All chunks processed."

# ---------------- assemble ----------------
if [[ -n "$ASSEMBLE" ]]; then
  echo "Assembling into $ASSEMBLE"
  : > "$ASSEMBLE"
  for ((i=0; i<PARTS; i++)); do
    base=$(printf "$PAD" "$i")
    f="${PREFIX}.${base}.part"
    [[ -f "$f" ]] || die "missing part: $f"
    cat "$f" >> "$ASSEMBLE"
  done
  echo "Assemble completed."
fi

